{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azlTxHYhH5wF",
    "tags": []
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uxvt030JHvWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in c:\\users\\antoi\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: edist in c:\\users\\antoi\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from stanza) (4.18.0)\n",
      "Requirement already satisfied: six in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from stanza) (1.22.4)\n",
      "Requirement already satisfied: protobuf in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from stanza) (3.20.0)\n",
      "Requirement already satisfied: requests in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from stanza) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from stanza) (4.64.0)\n",
      "Requirement already satisfied: emoji in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from stanza) (1.7.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from stanza) (1.11.0)\n",
      "Requirement already satisfied: proto-dist-ml in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from edist) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from edist) (1.1.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from edist) (1.7.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from requests->stanza) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from requests->stanza) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from requests->stanza) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from scikit-learn->edist) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from scikit-learn->edist) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from tqdm->stanza) (0.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from transformers->stanza) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from transformers->stanza) (3.6.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from transformers->stanza) (0.0.49)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from transformers->stanza) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from transformers->stanza) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from transformers->stanza) (0.5.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from transformers->stanza) (0.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers->stanza) (3.0.4)\n",
      "Requirement already satisfied: click in c:\\users\\antoi\\anaconda3\\lib\\site-packages (from sacremoses->transformers->stanza) (8.0.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install stanza edist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "oor18Vj3IHTj"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.drawing import nx_agraph\n",
    "import pydot\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import stanza\n",
    "\n",
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "import edist.tree_utils as tree_utils\n",
    "import edist.tree_edits as tree_edits\n",
    "import edist.ted as ted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pmRyUXGvIPQs"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb682b4083bb4539838b95088ce462c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 11:50:04 INFO: Downloading default packages for language: en (English)...\n",
      "2022-09-19 11:50:06 INFO: File exists: C:\\Users\\antoi\\stanza_resources\\en\\default.zip\n",
      "2022-09-19 11:50:09 INFO: Finished downloading models and saved to C:\\Users\\antoi\\stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948d93e6a7684599bff90fa506af8176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 11:50:10 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| lemma        | combined |\n",
      "| depparse     | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2022-09-19 11:50:10 INFO: Use device: cpu\n",
      "2022-09-19 11:50:10 INFO: Loading: tokenize\n",
      "2022-09-19 11:50:10 INFO: Loading: pos\n",
      "2022-09-19 11:50:11 INFO: Loading: lemma\n",
      "2022-09-19 11:50:11 INFO: Loading: depparse\n",
      "2022-09-19 11:50:11 INFO: Loading: constituency\n",
      "2022-09-19 11:50:11 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Build a Neural Pipeline\n",
    "stanza.download('en')\n",
    "StanzaPipeline = stanza.Pipeline('en', processors = \"tokenize,pos,lemma,depparse,constituency\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjJmtqwMIQOJ"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "d821e350-8953-4563-ab3e-b2811eb3b344"
   },
   "outputs": [],
   "source": [
    "def make_POS_basics(tree, dico):\n",
    "    \"\"\"\n",
    "    input :\n",
    "    - tree = classic NLTK tree, with X-POS and terminal leaves that are the words of the sentence\n",
    "    - dico = Stanza dictionnary https://stanfordnlp.github.io/stanza/data_objects.html#document\n",
    "    \n",
    "    returns :\n",
    "    - simple_tree = simplified NLTK tree, with U-POS and NO words from the sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    simple_tree = tree.copy(deep=True)\n",
    "    token_numero = 0\n",
    "    for position in tree.treepositions():\n",
    "        try: \n",
    "            subtree = tree[position]\n",
    "            if (type(subtree)==nltk.tree.Tree and subtree.height()==2) :\n",
    "                newPOS = dico[token_numero]['upos']\n",
    "                if ('\"' in newPOS) or ('``' in newPOS) :\n",
    "                  newPOS = 'QUOTESYMBOL'\n",
    "                newsubtree = Tree.fromstring('(' + newPOS + ')')\n",
    "                simple_tree[position] = newsubtree\n",
    "                token_numero += 1\n",
    "        except:\n",
    "            continue\n",
    "    return simple_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7gNhfxi9ten0"
   },
   "outputs": [],
   "source": [
    "def C_tree_to_dot(t):\n",
    "    \"\"\"\n",
    "    input :\n",
    "    t =  NLTK Tree, \n",
    "    \n",
    "    returns :\n",
    "    s = a dot representation, suitable for using with Graphviz\n",
    "    (type(s) = string for Python)\n",
    "    \"\"\"\n",
    "    def gv_print(t,start_node=1):\n",
    "        \"\"\"\n",
    "        Print the tree for a defined node. Nodes are specified in-order in the original tree\t\n",
    "        \"\"\"\n",
    "        # Print the start node of the tree\n",
    "        s ='%s [label=\"%s\"]' % (start_node,t.label())\n",
    "        pos=start_node+1\n",
    "\n",
    "\n",
    "        for child in t:\n",
    "            if isinstance(child,nltk.tree.Tree): # the node is the root of a subtree ( non terminal )\n",
    "                (s_child,newpos)=gv_print(child,pos)\n",
    "                s=s+'\\n'+ s_child\n",
    "                s=s+'\\n%s -> %s [dir=none]' % (start_node,pos)\n",
    "                pos=newpos\n",
    "            elif isinstance(child, str):  # the node is a leaf ( terminal )\n",
    "                s=s+'\\n%s [label=\"%s\"]' % (pos,child)\n",
    "                s=s+'\\n%s -> %s' % (start_node,pos)\t\n",
    "            pos+=1\n",
    "        return (s,pos-1)\n",
    "\n",
    "    # Print the digraph dot specification\n",
    "    s='digraph G{\\n'\t\n",
    "    s+=gv_print(t)[0]\n",
    "    s+=\"\\n}\"\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "w_IgUO_Dtouf"
   },
   "outputs": [],
   "source": [
    "def dot_to_nxgraph(path):\n",
    "    \"\"\"\n",
    "    input :\n",
    "    - path = directory + filename, filename is the name of the saved dot file\n",
    "    \n",
    "    returns :\n",
    "    - final_G1 = a NeworkX Digraph, not rooted\n",
    "    \"\"\"\n",
    "    G1 = nx.Graph(nx.nx_pydot.read_dot(path))\n",
    "    label1 = nx.get_node_attributes(G1, 'label')\n",
    "    final_G1 = nx.DiGraph()\n",
    "    \n",
    "    for vertex in G1.nodes.data('label'):\n",
    "        final_G1.add_node(vertex[0], label=vertex[1])\n",
    "    for edge in G1.edges.data():\n",
    "        final_G1.add_edge(edge[0], edge[1])\n",
    "\n",
    "    return final_G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WMorD1UUwkyO"
   },
   "outputs": [],
   "source": [
    "def plot_tree(T1, path):\n",
    "  \"\"\"\n",
    "  input :\n",
    "  - T1 = a NetorkX tree,\n",
    "  - path = directory + filename\n",
    "\n",
    "  returns :\n",
    "  None, saves a png @ path\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure(figsize=(8,8))\n",
    "  pos = graphviz_layout(T1, prog=\"dot\")\n",
    "  f = nx.draw_networkx(T1, pos, arrows=False, with_labels=True, font_size=8) \n",
    "  f = nx.draw_networkx_labels(T1, pos, nx.get_node_attributes(T1, 'label'), verticalalignment='bottom', horizontalalignment='right', font_size=13, font_color='r',)\n",
    "  plt.savefig(path+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SQ_Y648vKQaZ"
   },
   "outputs": [],
   "source": [
    "def graph_n_tree(phrase, path, display=False):\n",
    "  \"\"\"\n",
    "  input : \n",
    "  - phrase = string sentence\n",
    "  - path =  directory + filename, filename is the name of the saved dot file\n",
    "  - display = if True, displays the NLTK tree\n",
    "\n",
    "  returns :\n",
    "  - sent_dico = Stanza dictionnary https://stanfordnlp.github.io/stanza/data_objects.html#document\n",
    "  - dot_tree = the dot file (type(dot_tree) = string for Python),\n",
    "  - G = a NetworkX Directed Graph\n",
    "  - T = a NetworkX Tree\n",
    "  \"\"\"\n",
    "  doc = StanzaPipeline(phrase) #Stanza.Document\n",
    "  sentence = doc.sentences[0]  #Stanza.Sentence\n",
    "  sent_dico = sentence.to_dict()  #dictionnary\n",
    "  stanza_tree = sentence.constituency #Stanza.Tree\n",
    "  nltk_tree = Tree.fromstring(str(stanza_tree)) #NLTK.Tree\n",
    "\n",
    "  if display :\n",
    "    nltk_tree.pretty_print()\n",
    "\n",
    "  dot_tree = C_tree_to_dot(nltk_tree)\n",
    "  f = open(path+'.dot', 'w')\n",
    "  f.write(dot_tree)\n",
    "  f.close()\n",
    "  G = dot_to_nxgraph(path+'.dot')\n",
    "  T = nx.dfs_tree(G, source='1',)\n",
    "  nx.set_node_attributes(T, dict(G.nodes(data=True)))\n",
    "\n",
    "  return sent_dico, dot_tree, G, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ihdAb-OhTDF2"
   },
   "outputs": [],
   "source": [
    "def write_ted_dot(span1, span2, dot1, dot2, m_score, c_score, edit_seq, path):\n",
    "  \"\"\"\n",
    "  This function creates a .dot file to vizualise the mapping between the two trees.\n",
    "  Relies on the .dot files of each textspan, which are basically concatenated.\n",
    "  On top of that, a title and metrics is prepend, and the mapping is append\n",
    "  (Deleted and Inserted nodes are marqued in red, Mapped ones in green).\n",
    "  \n",
    "  input :\n",
    "  - span1/span2 = string textspan 1/2\n",
    "  - dot1/dot2 = dot descriptions of textspan 1/2 (obtained from graph_n_tree())\n",
    "  - m_score = first metric to display\n",
    "  - c_score = second metric to display\n",
    "  - edit_seq = edit path obtained during TED process @ format [(1,1), (2,4), (3, None) ...]\n",
    "  - path =  directory + filename, filename is the name of the saved dot file\n",
    "  \n",
    "  returns :\n",
    "  Nothing\n",
    "  \"\"\"  \n",
    "  #zss_r = compute_zss(line.G1, line.G2)\n",
    "  string = 'digraph G {\\n'\n",
    "  string += 'label=\"'+ span1.replace('\"','') + '\\n'\n",
    "  string += span2.replace('\"','') + '\\n'\n",
    "  string += 'RelSim = ' + str(round(m_score, 3)) + '\\n'\n",
    "  string += 'RelComp = ' + str(round(c_score, 3)) + '\"\\n'\n",
    "  string += 'labelloc = t; fontname = \"Helvetica,Arial,sans-serif\"; fontsize = 20; \\n'\n",
    "  string += 'overlap = false; \\n'\n",
    "  string += 'splines = true; \\n'\n",
    "  string += 'subgraph cluster0 {\\n'\n",
    "  string += 'style=filled; color=lightgrey; label = \"span 1\"\\n'\n",
    "  contentG1 = dot1[11:-1]\n",
    "  list_of_str = re.findall(\"[0-9]+\", contentG1)\n",
    "  list_of_int = [int(s) for s in list_of_str]\n",
    "  max = np.max(list_of_int)\n",
    "  string += contentG1 + '}\\n'\n",
    "\n",
    "  string += 'subgraph cluster1 { \\n'\n",
    "  string += 'style=filled; color=lightgrey; label = \"span 2\"\\n'\n",
    "  contentG2 = dot2[11:-1]\n",
    "  def add(re_match):\n",
    "    begin = re_match.span()[0]\n",
    "    end = re_match.span()[-1]\n",
    "    new = int(contentG2[begin:end]) + max\n",
    "    return str(new)\n",
    "  new_contentG2 = re.sub(\"[0-9]+\", add, contentG2)\n",
    "  string += new_contentG2 +'}\\n'\n",
    "  \n",
    "  for u,v in edit_seq[1:] :\n",
    "            if u == None and v != None :\n",
    "                string += str(int(v)+max) + ' [color=firebrick3] \\n'\n",
    "            elif v == None : \n",
    "                string += str(u) + ' [color=firebrick3] \\n'\n",
    "            else :\n",
    "                string += str(u) + ' -> ' + str(int(v)+max) + ' [color=chartreuse3, constraint=false] \\n'\n",
    "\n",
    "\n",
    "  string += '}'\n",
    "\n",
    "  f = open(path+'.dot', 'w')\n",
    "  f.write(string)\n",
    "  f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Ev4NG-SBRpcZ"
   },
   "outputs": [],
   "source": [
    "def get_path_edist(ALIGN):\n",
    "  \"\"\"\n",
    "  input :\n",
    "  - ALIGN = backtracing sequence at EDIST format https://edist.readthedocs.io/en/latest/ted.html\n",
    "  \n",
    "  returns :\n",
    "  - the list of tuple of the match necessary for EDIST library @ format [(1,1), (2,4), (3, None) ...]\n",
    "  \"\"\"\n",
    "  NUM_SEQ = []\n",
    "  for TUPLE in ALIGN :\n",
    "    left = TUPLE._left +1\n",
    "    right = TUPLE._right +1\n",
    "    if left == 0 :\n",
    "      left = None\n",
    "      right = str(right)\n",
    "    elif right == 0 :\n",
    "      right = None\n",
    "      left = str(left)\n",
    "    else :\n",
    "      left = str(left)\n",
    "      right = str(right)\n",
    "\n",
    "    NUM_SEQ.append((left,right))\n",
    "  \n",
    "  return NUM_SEQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6wG7pRF_VrIf"
   },
   "outputs": [],
   "source": [
    "def nxTree_to_edist(TREE):\n",
    "  \"\"\"\n",
    "  input :\n",
    "  - TREE = NetworkX tree\n",
    "  \n",
    "  returns :\n",
    "  (two elements for creating a tree at EDIST format:\n",
    "  https://gitlab.ub.uni-bielefeld.de/bpaassen/python-edit-distances/-/tree/master/ )\n",
    "  - list_label = the node list\n",
    "  - adj_list = the adjacency list\n",
    "  \"\"\"\n",
    "  list_label = []\n",
    "  for NODE in TREE.nodes :\n",
    "    list_label.append(TREE.nodes[NODE]['label'][1:-1])\n",
    "\n",
    "\n",
    "  adj_list = []\n",
    "  for CHILDREN in nx.generate_adjlist(TREE):\n",
    "    NEIGHBORS = re.findall(\"[0-9]+\",CHILDREN)\n",
    "    NEIGHBORS = [int(s)-1 for s in NEIGHBORS]\n",
    "    adj_list.append(NEIGHBORS[1:])\n",
    "  \n",
    "  return list_label, adj_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L48zgMY-ISqM"
   },
   "source": [
    "# sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GjeVKYFKt60U"
   },
   "outputs": [],
   "source": [
    "arg1 = \"Nick makes the most delicious cakes\"\n",
    "arg2 = \"and Grace makes the most disgusting desserts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "yAEzFBmJwP1B"
   },
   "outputs": [],
   "source": [
    "arg3= \"Nick is excellent at baking\"\n",
    "arg4 = \"and Grace makes the most disgusting desserts.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyEvHZE9IVlP"
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxfr5foDK3q3",
    "outputId": "8045022b-45f7-48a0-d663-9afc718e0e97"
   },
   "outputs": [],
   "source": [
    "def pipeline(spanA, spanB, PATH, DISPLAY=False):\n",
    "    \"\"\"\n",
    "    this function compiles all above functions in order to compute the TED\n",
    "    \n",
    "    input :\n",
    "    - spanA/spanB = textspan 1/2\n",
    "    - PATH = concerns the mappring: directory + filename\n",
    "    - display = to display the *full* syntax trees in the script\n",
    "    \n",
    "    returns:\n",
    "    - RelSim and RelComp, or any metric of your choice\n",
    "    \"\"\"\n",
    "    \n",
    "    dictionnary1, dot1, G1, T1 = graph_n_tree(spanA, 'CFspan1', display=DISPLAY)\n",
    "    dictionnary2, dot2, G2, T2 = graph_n_tree(spanB, 'CFspan2', display=DISPLAY)\n",
    "\n",
    "    x_nodes, x_adj = nxTree_to_edist(T1)\n",
    "    y_nodes, y_adj = nxTree_to_edist(T2)\n",
    "\n",
    "    # define the distance between nodes in algebraic expressions\n",
    "    def delta(x, y):\n",
    "      if (x is None or y is None) :\n",
    "        return 1.\n",
    "      else :\n",
    "        if(x == y):\n",
    "          return 0.\n",
    "        else:\n",
    "          return 1e6 # or np.inf\n",
    "    \n",
    "    # ted.ted only computes the TED metric\n",
    "    #TED = ted.ted(x_nodes, x_adj, y_nodes, y_adj, delta)\n",
    "    \n",
    "    # while ted.ted_backtrace returns the mapping for nodes of T1 to nodes of T2\n",
    "    ALIGN = ted.ted_backtrace(x_nodes, x_adj, y_nodes, y_adj, delta)\n",
    "    NUM_SEQ = get_path_edist(ALIGN)\n",
    "\n",
    "    # To compute RelSim\n",
    "    MatchedPairs = [pair for pair in NUM_SEQ[1:] if None not in pair]\n",
    "    M = len(MatchedPairs)\n",
    "    RelSim = M / min(G1.size(), G2.size())\n",
    "    \n",
    "    # To compute RelComp\n",
    "    NodesSubset1 = [pair[0] for pair in MatchedPairs if (pair[0] != '1')]\n",
    "    NodesSubset2 = [pair[1] for pair in MatchedPairs if (pair[0] != '1')]\n",
    "    graph1 = G1.to_undirected()\n",
    "    graph2 = G2.to_undirected()  \n",
    "    forest1 = graph1.subgraph(NodesSubset1)\n",
    "    forest2 = graph2.subgraph(NodesSubset2)\n",
    "    ConnectedComponents1 = list(nx.connected_components(forest1))\n",
    "    ConnectedComponents2 = list(nx.connected_components(forest2))\n",
    "    ConnectedComponents1.sort(key = len)\n",
    "    ConnectedComponents2.sort(key = len)\n",
    "    RelComp = min(len(ConnectedComponents1[-1]),\n",
    "            len(ConnectedComponents2[-1])) / M \n",
    "    \n",
    "    # to get a representation of the mapping\n",
    "    write_ted_dot(spanA, spanB, dot1, dot2, \n",
    "                  edit_seq=NUM_SEQ, \n",
    "                  c_score = RelComp, m_score = RelSim, \n",
    "                  path = PATH) \n",
    "    (graph,) = pydot.graph_from_dot_file(PATH+'.dot')\n",
    "    graph.write_png(PATH+'.png')\n",
    "    #Image(path + 'compare.png' )\n",
    "    \n",
    "    return RelSim, RelComp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ROOT                      \n",
      "             |                         \n",
      "             S                        \n",
      "  ___________|______                   \n",
      " |                  VP                \n",
      " |     _____________|___               \n",
      " |    |                ADJP           \n",
      " |    |       __________|____          \n",
      " |    |      |               PP       \n",
      " |    |      |           ____|____     \n",
      " NP   |      |          |         VP  \n",
      " |    |      |          |         |    \n",
      "NNP  VBZ     JJ         IN        NN  \n",
      " |    |      |          |         |    \n",
      "Nick  is excellent      at      baking\n",
      "\n",
      "                ROOT                                  \n",
      "                 |                                     \n",
      "                 S                                    \n",
      "  _______________|__________________________________   \n",
      " |    |          VP                                 | \n",
      " |    |      ____|____                              |  \n",
      " |    |     |         NP                            | \n",
      " |    |     |     ____|______________________       |  \n",
      " |    NP    |    |        ADJP               |      | \n",
      " |    |     |    |     ____|_______          |      |  \n",
      " CC  NNP   VBZ   DT  RBS           JJ       NNS     . \n",
      " |    |     |    |    |            |         |      |  \n",
      "and Grace makes the  most      disgusting desserts  . \n",
      "\n",
      "\n",
      "=========================================\n",
      "Relative similarity =  0.4375\n",
      "Relative compacity =  0.7142857142857143\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "RelSim, RelComp = pipeline(arg3, arg4, PATH = 'notcakes', DISPLAY=True)\n",
    "print('\\n=========================================')\n",
    "print('Relative similarity = ', RelSim)\n",
    "print('Relative compacity = ', RelComp)\n",
    "print('=========================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "cocina.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
